import { supabase } from "@/lib/supabase";
import { NextRequest, NextResponse } from "next/server";
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// const testSql = `
// SELECT
//   to_json(T1.id) AS id,
//   T1.name,
//   T1.headline,
//   T1.location,
//   T1.publications,
//   T1.bio,
//   T1.linkedin_url,
//   T1.total_exp_months,
//   T1.links,
//   T1.profile_picture,
//   jsonb_agg(
//     DISTINCT jsonb_build_object(
//         'experience', jsonb_strip_nulls(to_jsonb(T2.*)),
//         'company', jsonb_strip_nulls(
//             jsonb_build_object(
//                 'logo', T3.logo,
//                 'name', T3.name
//             )
//         )
//     )
//   ) FILTER (WHERE T2.candid_id IS NOT NULL) AS experiences,
//   jsonb_agg(
//       DISTINCT jsonb_strip_nulls(to_jsonb(T4.*))
//   ) FILTER (WHERE T4.candid_id IS NOT NULL) AS educations
// FROM
//   candid AS T1
// INNER JOIN
//   experience_user AS T2 ON T1.id = T2.candid_id
// INNER JOIN
//   company_db AS T3 ON T2.company_id = T3.id
// INNER JOIN
//   edu_user AS T4 ON T1.id = T4.candid_id
// ${parsed_text}

// GROUP BY
//   T1.id
// `

async function parseQueryWithLLM(queryText: string): Promise<string> {
  const prompt = `
ë„ˆëŠ” ì±„ìš© í›„ë³´ìž ê²€ìƒ‰ì„ ìœ„í•œ **ì „ë¬¸ SQL Query Parser**ë‹¤.
ìž…ë ¥ì€ ìžì—°ì–´ë¡œ ìž‘ì„±ëœ ì±„ìš© í›„ë³´ìž ê²€ìƒ‰ ìš”ì²­ì´ë‹¤.

ë„ˆì˜ ëª©í‘œëŠ”:
- ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ **í›„ë³´ìžê°€ ì¶©ë¶„ížˆ ë§Žì´ ë§¤ì¹­ë˜ë„ë¡**
- ì¡°ê±´ì„ **ë„ˆë¬´ ì¢ížˆì§€ ë§ê³ **, í•©ë¦¬ì ìœ¼ë¡œ **í™•ìž¥ëœ OR ì¡°ê±´**ì„ ì‚¬ìš©í•˜ì—¬
- ê²€ìƒ‰ í’ˆì§ˆì´ ì¢‹ì€ WHERE ì ˆì„ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.

---

### ðŸ“¦ Database Schema

- T1: candid  
  - id (PK)
  - headline
  - bio
  - name
  - location

- T2: experience_user  
  - candid_id (FK â†’ candid.id)
  - role
  - description : ë³¸ì¸ì´ í•œ ì¼ì—ëŒ€í•œ ì„¤ëª…
  - start_date (DATE, format: YYYY-MM-DD)
  - end_date (DATE)
  - company_id (FK â†’ company_db.id)

- T3: company_db  
  - id (PK)
  - name
  - description : íšŒì‚¬ì— ëŒ€í•œ ì„¤ëª…
  - employee_count_range
  - founded_year
  - website_url

- T4: education_user  
  - candid_id (FK â†’ candid.id)
  - school
  - degree
  - field
  - start_date (DATE)
  - end_date (DATE)

---

### ðŸš¨ ì¶œë ¥ ê·œì¹™ (ì ˆëŒ€ ìœ„ë°˜ ê¸ˆì§€)

1. **ì¶œë ¥ì€ ë°˜ë“œì‹œ WHERE ë¡œ ì‹œìž‘í•˜ëŠ” SQL ì¡°ê±´ë¬¸ë§Œ ë°˜í™˜í•œë‹¤**
   - SELECT, FROM, JOIN, ORDER BY, LIMIT í¬í•¨ No
   - ì˜¤ì§ 'WHERE ...' ë³¸ë¬¸ë§Œ ì¶œë ¥

2. **ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€ SQL**
   - UPDATE No
   - DELETE No
   - INSERT No
   - DROP No

3. **ì¡°ê±´ í‘œí˜„ ë°©ì‹**
   - ëª¨ë“  ì¡°ê±´ì€ ë°˜ë“œì‹œ 'ILIKE '%keyword%'' í˜•ì‹ ì‚¬ìš©
   - ì •í™• ì¼ì¹˜('='), ì •ê·œì‹('~'), full-text search No
   - ë°˜ë“œì‹œ 'OR / AND' ì¡°í•©ìœ¼ë¡œ ìž‘ì„±

4. **ì–¸ì–´ ê·œì¹™**
   - ë°ì´í„°ëŠ” **ëŒ€ë¶€ë¶„ ì˜ì–´**ë¡œ ì €ìž¥ë˜ì–´ ìžˆìŒ
   - í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•  ê²½ìš°:
     - ë°˜ë“œì‹œ ëŒ€ì‘ë˜ëŠ” ì˜ì–´ í‚¤ì›Œë“œë¥¼ **í•¨ê»˜ OR ì¡°ê±´ìœ¼ë¡œ í¬í•¨**
     - (ì˜ˆ: "ì„œìš¸ëŒ€í•™êµ" â†’ "seoul national university", "SNU")

---

### ðŸŽ¯ ê²€ìƒ‰ ì „ëžµ ê°€ì´ë“œ (ë§¤ìš° ì¤‘ìš”)

- ì¡°ê±´ì„ **í•œë‘ ê°œë§Œ ì“°ì§€ ë§ê³ **, ë°˜ë“œì‹œ **ì—¬ëŸ¬ ê°œì˜ í™•ìž¥ëœ í‚¤ì›Œë“œ**ë¥¼ ì‚¬ìš©í•˜ë¼
- ë„ˆë¬´ íƒ€ì´íŠ¸í•œ AND ì¡°ê±´ì„ ë‚¨ë°œí•˜ì§€ ë§ ê²ƒ
- ê°€ëŠ¥í•˜ë©´ ë‹¤ìŒì„ ì ê·¹ í™œìš©í•˜ë¼:
  - ì§ë¬´ ìœ ì‚¬ì–´ (engineer / scientist / researcher / developer ë“±)
  - ì „ê³µ ìœ ì‚¬ì–´ (computer science / software / AI / ML / data ë“±)
  - íšŒì‚¬ ì„¤ëª…(description) ê¸°ë°˜ ê²€ìƒ‰
  - headline / bio / publications í™œìš©

---

### ðŸ§  ì¡°ê±´ í•´ì„ ê°€ì´ë“œ

- í•™ë ¥ ì¡°ê±´ â†’ T4.school, T4.degree, T4.field
- ì§ë¬´/ê²½ë ¥ â†’ T2.role, T2.description
- íšŒì‚¬ íŠ¹ì§• â†’ T3.name, T3.description
- ê°œì¸ í‚¤ì›Œë“œ â†’ T1.headline, T1.bio, T1.publications, T1.location

---

### ðŸ“… ë‚ ì§œ ì¡°ê±´ (ì„ íƒì )

- ê²½ë ¥ ì—°ì°¨, ìµœê·¼ ê·¼ë¬´ ì—¬ë¶€ê°€ í¬í•¨ëœ ê²½ìš°:
  - start_date / end_dateì— ëŒ€í•´
  - ì§ì ‘ ê³„ì‚°ì€ í•˜ì§€ ë§ê³ , **ì—°ë„ ë¬¸ìžì—´ ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰ì€ ê¸ˆì§€**
  - ë‚ ì§œ ì¡°ê±´ì´ ì• ë§¤í•˜ë©´ **ë‚ ì§œ ì¡°ê±´ì„ ìƒëžµí•˜ê³  ì§ë¬´ í‚¤ì›Œë“œë¡œ ë³´ì™„**

---

### âœ… ì¶œë ¥ ì˜ˆì‹œ

ìžì—°ì–´ ìž…ë ¥:
> í•œêµ­ Top ëŒ€í•™ ì¶œì‹ ì´ë©´ì„œ AI ë¦¬ì„œì²˜ ë˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´

ì¶œë ¥:
WHERE
(
  T4.school ILIKE '%seoul national university%'
  OR T4.school ILIKE '%SNU%'
  OR T4.school ILIKE '%ì„œìš¸ëŒ€í•™êµ%'
  OR T4.school ILIKE '%yonsei university%'
  OR T4.school ILIKE '%ì—°ì„¸ëŒ€í•™êµ%'
  OR T4.school ILIKE '%korea university%'
  OR T4.school ILIKE '%ê³ ë ¤ëŒ€í•™êµ%'
  OR T4.school ILIKE '%KAIST%'
  OR T4.school ILIKE '%postech%'
)
AND
(
  T4.field ILIKE '%computer%'
  OR T4.field ILIKE '%software%'
  OR T4.field ILIKE '%artificial intelligence%'
  OR T4.field ILIKE '%machine learning%'
)
OR
(
  T2.role ILIKE '%research%'
  OR T2.role ILIKE '%machine learning engineer%'
  OR T2.role ILIKE '%ml engineer%'
  OR T2.role ILIKE '%ai engineer%'
  OR T1.headline ILIKE '%research%'
)

---

### ðŸ“¥ ìž…ë ¥

Natural Language Query:
${queryText}

---

### âš ï¸ ë§ˆì§€ë§‰ ê²½ê³ 

- ì„¤ëª…, ì£¼ì„, ì½”ë“œë¸”ë¡, ë§ˆí¬ë‹¤ìš´ ì¶œë ¥ X
- SQL WHERE ì ˆ **ë³¸ë¬¸ë§Œ** ì¶œë ¥í•˜ë¼
- í•œ ì¤„ì´ë¼ë„ ê·œì¹™ì„ ì–´ê¸°ë©´ ì‹¤íŒ¨ë‹¤
`.trim();

  // Responses API + structured outputs (text.format)
  const resp = await openai.responses.create({
    model: "gpt-4.1", // ë„ˆ í™˜ê²½ì— ë§žê²Œ ì¡°ì •
    input: prompt,
  });

  // SDKê°€ ë°˜í™˜í•˜ëŠ” êµ¬ì¡°ëŠ” ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìžˆì–´,
  // ì•„ëž˜ëŠ” "ìµœì¢… í…ìŠ¤íŠ¸(JSON)"ë¥¼ êº¼ë‚´ëŠ” ë³´ìˆ˜ì ì¸ ë°©ì‹.
  const outText =
    resp.output_text ?? (resp as any).output?.[0]?.content?.[0]?.text ?? "";

  return outText;
}

export async function POST(req: NextRequest) {
  const body = await req.json();
  const { queryId, pageIdx } = body;
  console.log("ì§„ìž… í™•ì¸ : ", queryId, pageIdx);

  if (!queryId)
    return NextResponse.json({ error: "Missing queryId" }, { status: 400 });

  const { data: q, error: qErr } = await supabase
    .from("queries")
    .select("query_id,user_id,raw_input_text,query")
    .eq("query_id", queryId)
    .single();

  console.log("ì¼ë‹¨ ì¿¼ë¦¬ í™•ì¸ : ", q);

  if (qErr || !q || !q.raw_input_text)
    return NextResponse.json({ error: "Query not found" }, { status: 404 });

  const { data: results, error: lpErr } = await supabase
    .from("query_pages")
    .select("*")
    .eq("query_id", queryId)
    .eq("page_idx", pageIdx)
    .maybeSingle();

  console.log(pageIdx, "ì¿¼ë¦¬ì™€ results ", results, q);

  if (lpErr)
    return NextResponse.json({ error: lpErr.message }, { status: 500 });

  const nextPageIdx = pageIdx + 1;

  // ì €ìž¥ë˜ì–´ìžˆëŠ” ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ìƒˆë¡­ê²Œ ê²€ìƒ‰í•´ì•¼í•œë‹¤ëŠ” ëœ».
  if (!results) {
    let parsed_query = q.query;
    if (!parsed_query) {
      parsed_query = await parseQueryWithLLM(q.raw_input_text);
      console.log("parsed_text ", parsed_query);

      const upsertRes = await supabase
        .from("queries")
        .upsert({ query_id: queryId, user_id: q.user_id, query: parsed_query });
      console.log("upsertRes ", upsertRes);
    }

    // LLMì´ ìƒì„±í•´ì•¼ í•˜ëŠ” ì•ˆì „í•œ SQL ì¿¼ë¦¬ (ì˜ˆì‹œ)
    const testSql = `
SELECT 
  to_json(T1.id) AS id,
  T1.name,
  T1.headline,
  T1.location,
  T1.publications,
  T1.bio,
  T1.linkedin_url,
  T1.total_exp_months,
  T1.links,
  T1.profile_picture,
  jsonb_agg(
    DISTINCT jsonb_build_object(
        'experience', jsonb_strip_nulls(to_jsonb(T2.*)),
        'company', jsonb_strip_nulls(
            jsonb_build_object(
                'logo', T3.logo,
                'name', T3.name
            )
        )
    )
  ) FILTER (WHERE T2.candid_id IS NOT NULL) AS experiences,
  jsonb_agg(
      DISTINCT jsonb_strip_nulls(to_jsonb(T4.*))
  ) FILTER (WHERE T4.candid_id IS NOT NULL) AS educations
FROM 
  candid AS T1
INNER JOIN 
  experience_user AS T2 ON T1.id = T2.candid_id
INNER JOIN 
  company_db AS T3 ON T2.company_id = T3.id
INNER JOIN
  edu_user AS T4 ON T1.id = T4.candid_id
${parsed_query}

GROUP BY 
  T1.id
`;

    const limit = 10;
    const { data, error } = await supabase.rpc(
      "set_timeout_and_execute_raw_sql",
      {
        sql_query: testSql,
        page_idx: pageIdx,
        limit_num: limit,
      }
    );

    console.log("ê·¸ëž˜ì„œ ê²€ìƒ‰í•œê²Œ ë­”ë°? data ", data, error);

    const buildSummary = (doc: any) => {
      const exps = doc.experiences?.map((exp: any) => {
        let expText = `Role: ${exp.role}, Company: ${exp.company.name}`;
        if (exp.start_date) {
          expText += `, Start Date: ${exp.start_date}`;
        }
        if (exp.end_date) {
          expText += `, End Date: ${exp.end_date}`;
        }

        return expText;
      });

      const educations = doc.educations?.map((edu: any) => {
        let eduText = `School: ${edu.school}, Degree: ${edu.degree}, Field: ${edu.field}`;
        if (edu.start_date) {
          eduText += `, Start Date: ${edu.start_date}`;
        }
        if (edu.end_date) {
          eduText += `, End Date: ${edu.end_date}`;
        }
        return eduText;
      });

      const publications = doc.publications
        ? JSON.stringify(doc.publications.slice(0, 5))
        : "";

      const bio = doc.bio ? doc.bio : "";
      return `
${doc.name} is a ${doc.location} based.
About: ${bio}
Headline: ${doc.headline}
Experiences: ${exps}
Educations: ${educations}
Publications: ${publications}`;
    };

    data[0]?.forEach(async (doc: any, index: number) => {
      const res_check = await supabase
        .from("synthesized_summary")
        .select("*")
        .eq("candid_id", doc.id)
        .eq("query_id", queryId)
        .maybeSingle();

      console.log("res_check ", res_check, doc);

      if (!res_check.data) {
        const information = buildSummary(doc);

        const res = await openai.chat.completions.create({
          model: "gpt-4.1-nano",
          messages: [
            {
              role: "system",
              content:
                "You are a helpful assistant. Given a search query and a candidate profile, generate a relevance-focused summary explaining why this candidate matches the query. Use exactly three sentences. Highlight especially important skills, experiences, or keywords by wrapping them with <strong> tags. ì˜ì–´ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ëŠ”ê±´ ìƒê´€ì—†ëŠ”ë°, í•œê¸€ë¡œ ëŒ€ë‹µí•´ì¤˜.",
            },
            {
              role: "user",
              content: `Search Query : ${q.raw_input_text} \n\n Information : ${information}`,
            },
          ],
        });
        const summary = res.choices[0].message.content;
        console.log("summary ", summary);

        const { error: insErr } = await supabase
          .from("synthesized_summary")
          .insert({
            candid_id: doc.id,
            query_id: queryId,
            text: summary,
          });
      } else {
      }

      if (index === data[0].length - 1) {
      }
    });

    const candidateIds = data[0]?.map((r: any) => r.id) ?? [];
    console.log("ê²€ìƒ‰ ê²°ê³¼ candidateIds ", candidateIds);

    const { error: insErr } = await supabase.from("query_pages").insert({
      query_id: queryId,
      page_idx: pageIdx,
      candidate_ids: candidateIds,
    });

    if (insErr)
      return NextResponse.json({ error: insErr.message }, { status: 500 });

    return NextResponse.json(
      { nextPageIdx, results: candidateIds },
      { status: 200 }
    );
  } else {
    return NextResponse.json(
      { nextPageIdx, results: results.candidate_ids },
      { status: 200 }
    );
  }
}
