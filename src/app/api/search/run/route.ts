import { geminiInference, xaiInference } from "@/lib/llm/llm";
import { supabase } from "@/lib/supabase";
import { ensureGroupBy } from "@/utils/textprocess";
import { NextRequest, NextResponse } from "next/server";
import { generateSummary } from "../criteria_summarize/utils";
import {
  deduplicateAndScore,
  mapWithConcurrency,
  ScoredCandidate,
  sumScore,
} from "../utils";
import { ko } from "@/lang/ko";
import { logger } from "@/utils/logger";
import { updateRunStatus } from "../utils";
import { parseQueryWithLLM } from "../parse";

export const UI_START = "<<UI>>";
export const UI_END = "<<END_UI>>";

type RunRow = {
  id: string;
  query_id: string;
  criteria?: any | null; // jsonb
  sql_query?: string | null;
  query_text?: string | null;
};

/**
 * Search the database and return scored candidates.
 * - Stores synthesized summaries in bulk.
 * - Does NOT write runs_pages here; caller decides how to chunk/cache.
 */
export const searchDatabase = async (
  query_text: string,
  criteria: string[],
  pageIdx: number,
  run: RunRow,
  sql_query: string,
  limit: number = 50,
  offset: number = 0
) => {
  await updateRunStatus(run.id, ko.loading.searching_candidates);

  const start_time = performance.now();

  let data: any[] | null = [];
  let error: any;

  const { data: data1, error: error1 } = await supabase.rpc(
    "set_timeout_and_execute_raw_sql",
    {
      sql_query,
      page_idx: pageIdx,
      limit_num: limit,
      offset_num: offset,
    }
  );

  data = data1;
  error = error1;

  logger.log("time for fetching data:", performance.now() - start_time, error);

  // Retry once on timeout
  if (error && String(error.message || "").includes("timeout")) {
    await updateRunStatus(run.id, ko.loading.searching_again);

    const { data: data2, error: error2 } = await supabase.rpc(
      "set_timeout_and_execute_raw_sql",
      {
        sql_query,
        page_idx: pageIdx,
        limit_num: limit,
        offset_num: offset,
      }
    );

    data = data2;
    error = error2;
  }

  // Fix query on error (timeout or syntax)
  if (error) {
    logger.log("First sql query error => try fix:", error);

    await updateRunStatus(run.id, ko.loading.retrying_error);

    let additional_prompt = "";
    if (String(error.message || "").includes("timeout")) {
      additional_prompt = `
If the error indicates a timeout, treat it as a performance-fix task rather than a syntax-fix task.

TIMEOUT rules:
- Preserve meaning/rows as much as possible; restructure only for speed.
- Prefer two-phase approach: select only T1.id with restrictive filters + LIMIT, then join to fetch final columns.
- Do NOT add new tables/filters or change ranking semantics.
- Replace JOIN-based filtering with EXISTS when joins are only for filtering.
- Push down WHERE filters into phase-1 id CTE.
- Output MUST be a single valid SQL statement only.
`;
    }

    const fixed_query = await xaiInference(
      "grok-4-fast-reasoning",
      "You are a specialized SQL query fixing assistant. Fix errors and return a single SQL statement only.",
      `You are an expert PostgreSQL SQL fixer for a recruitment candidate search system.

Rules:
- Fix ONLY what is necessary.
- Preserve original intent/meaning.
- Do NOT add new tables/filters unless required to fix the error.
- Keep tsvector logic in place.
- Output MUST be a single valid SQL statement only. No explanations.

${additional_prompt}

[SQL]
${sql_query}

[ERROR]
${error.message}
`,
      0.2,
      1
    );

    const sqlQueryWithGroupBy2 = ensureGroupBy(fixed_query as string, "");

    // Save fixed SQL to the run (optional)
    await supabase
      .from("runs")
      .update({ sql_query: fixed_query as string })
      .eq("id", run.id);

    const { data: data2, error: error2 } = await supabase.rpc(
      "set_timeout_and_execute_raw_sql",
      {
        sql_query: sqlQueryWithGroupBy2,
        page_idx: pageIdx,
        limit_num: limit,
        offset_num: offset,
      }
    );

    data = data2;
    error = error2;
  }

  if (error) {
    await updateRunStatus(
      run.id,
      `Error: ${String(error.message || error)}` || "Error"
    );
    return [];
  }

  if (!data || !data[0] || data[0].length === 0) {
    await updateRunStatus(run.id, "No data found");
    return [];
  }

  await updateRunStatus(run.id, ko.loading.summarizing);

  const fullScore = criteria.length * 2;

  const scored: (ScoredCandidate & { summary: string })[] =
    await mapWithConcurrency(data[0] as any[], 17, async (candidate) => {
      const id = candidate.id as string;

      let summary = "";
      let lines: string[] = [];
      try {
        summary = (await generateSummary(
          candidate,
          criteria,
          query_text
        )) as string;
        lines = JSON.parse(summary);
      } catch {
        summary = "";
        lines = [];
      }

      const score = sumScore(lines);

      return {
        id,
        score:
          fullScore !== 0 ? Math.round((score / fullScore) * 100) / 100 : 1,
        summary,
      };
    });

  logger.log("ðŸ’  ì „ì²´ ìŠ¤ì½”ì–´ë§ ì™„ë£Œ: ", scored);
  // Bulk upsert synthesized summaries
  // Prefer run_id if the column exists; fallback to query_id if not.
  const upsertData = scored
    .filter((s) => s.summary != null)
    .map((s) => ({
      candid_id: s.id,
      run_id: run.id, // <-- recommended column\
      text: s.summary,
    }));

  if (upsertData.length > 0) {
    // If your synthesized_summary table does NOT have run_id yet,
    // remove run_id from upsertData above and keep only query_id.
    const { error: upErr } = await supabase
      .from("synthesized_summary")
      .upsert(upsertData as any);
    if (upErr) logger.log("Batch upsert synthesized_summary error:", upErr);
  }

  // Sort by score desc (stable-ish by id)
  scored.sort((a, b) => b.score - a.score || a.id.localeCompare(b.id));

  await updateRunStatus(run.id, "Done");

  return scored;
};

export async function POST(req: NextRequest) {
  const body = await req.json();
  const { queryId, runId, pageIdx, userId } = body as {
    queryId?: string;
    runId?: string;
    pageIdx?: number;
    userId?: string;
  };
  logger.log("\nìš°ì„  ì—¬ê¸°ì„œ í˜¸ì¶œ : ", body, "\n\n");

  if (!queryId || !runId) {
    return NextResponse.json(
      { error: "Missing queryId/runId" },
      { status: 400 }
    );
  }

  const page = Number.isFinite(pageIdx) ? (pageIdx as number) : 0;
  const nextPageIdx = page + 1;

  // 1) Check cached page in runs_pages
  const { data: cachedPages, error: cpErr } = await supabase
    .from("runs_pages")
    .select("*")
    .eq("run_id", runId)
    .eq("page_idx", page)
    .order("created_at", { ascending: false });

  if (cpErr) {
    logger.log("runs_pages load error:", cpErr);
  }

  const cached = cachedPages?.[0];
  if (cached && cached.candidate_ids) {
    const candidateIds = cached.candidate_ids
      .slice(0, 10)
      .map((r: any) => r.id);
    return NextResponse.json(
      { nextPageIdx, results: candidateIds },
      { status: 200 }
    );
  }

  // 2) If no cached results and page > 0, check previous page to decide if we can "slice" without new search
  let offset = 0;
  let cachedCandidates: any[] = [];

  if (page > 0) {
    const { data: prevPages } = await supabase
      .from("runs_pages")
      .select("*")
      .eq("run_id", runId)
      .eq("page_idx", page - 1)
      .order("created_at", { ascending: false });

    const prev = prevPages?.[0];

    if (!prev || !prev.candidate_ids || prev.candidate_ids.length === 0) {
      return NextResponse.json({ nextPageIdx, results: [] }, { status: 200 });
    }

    const isLoadMore = (prev.candidate_ids.length + page * 10) % 50 === 0;

    if (!isLoadMore) {
      // Just slice next 10 from prev cached candidates
      const rest = prev.candidate_ids.slice(10);

      await supabase.from("runs_pages").insert({
        run_id: runId,
        page_idx: page,
        candidate_ids: rest,
      });

      return NextResponse.json(
        { nextPageIdx, results: rest.slice(0, 10).map((r: any) => r.id) },
        { status: 200 }
      );
    } else {
      // It's a 50 boundary - decide if we can still slice or need new search
      const rest = prev.candidate_ids.slice(10);
      const scoreSum = rest
        .slice(0, 10)
        .reduce((acc: number, curr: any) => acc + curr.score, 0);

      if (scoreSum >= 10) {
        await supabase.from("runs_pages").insert({
          run_id: runId,
          page_idx: page,
          candidate_ids: rest,
        });

        return NextResponse.json(
          { nextPageIdx, results: rest.slice(0, 10).map((r: any) => r.id) },
          { status: 200 }
        );
      } else {
        offset = 50;
        cachedCandidates = rest;
      }
    }
  }

  // 3) Load run (source of truth for raw_input_text / criteria / sql_query)
  const { data: run, error: rErr } = await supabase
    .from("runs")
    .select("id, query_id, query_text, criteria, sql_query")
    .eq("id", runId)
    .eq("query_id", queryId)
    .single();

  if (rErr || !run) {
    return NextResponse.json({ error: "Run not found" }, { status: 404 });
  }

  let query_text = run.query_text ?? "";
  // 5) Ensure criteria/sql_query exist on the run (create if missing)
  let criteria: string[] = Array.isArray(run.criteria)
    ? (run.criteria as any)
    : [];
  let sql_query: string | null = run.sql_query ?? null;

  // If criteria is stored as object {criteria:[],...} in jsonb, normalize here
  // Example: run.criteria = { criteria: [...], thinking: "...", rephrasing: "..." }
  if (!criteria.length && run.criteria && typeof run.criteria === "object") {
    const maybe = (run.criteria as any)?.criteria;
    if (Array.isArray(maybe)) criteria = maybe;
  }

  if (!sql_query) {
    await updateRunStatus(run.id, ko.loading.making_query);

    const parsedQuery = await parseQueryWithLLM(query_text, criteria, "");
    if (typeof parsedQuery !== "string") {
      await updateRunStatus(run.id, JSON.stringify(parsedQuery));
      return NextResponse.json(parsedQuery, { status: 404 });
    }

    sql_query = parsedQuery;
    await supabase
      .from("runs")
      .update({
        sql_query: sql_query,
      })
      .eq("id", run.id);
  }

  // 6) Run search
  const searchResults = await searchDatabase(
    query_text,
    criteria,
    page,
    run as RunRow,
    sql_query,
    50,
    offset
  );

  const oneScoreCount = searchResults.filter((r: any) => r.score === 1).length;
  const merged = deduplicateAndScore(searchResults, cachedCandidates);

  const defaultMsg = `ì „ì²´ í›„ë³´ìžë“¤ ì¤‘ ${searchResults.length}ëª…ì„ ì„ ì •í•˜ê³ , ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ”ì§€ ê²€ì‚¬í–ˆìŠµë‹ˆë‹¤. ${oneScoreCount}ëª…ì´ ëª¨ë“  ê¸°ì¤€ì„ ë§Œì¡±í–ˆìŠµë‹ˆë‹¤.
${UI_START}{"type": "search_result", "text": "ê²€ìƒ‰ ê²°ê³¼", "run_id": "${runId}"}${UI_END}`;

  const msg = await xaiInference(
    "grok-4-fast-reasoning",
    "You are a helpful assistant.",
    `ì°¾ìœ¼ë ¤ëŠ” ì‚¬ëžŒì€ ${run.query_text}ìž…ë‹ˆë‹¤.
  ì§€ê¸ˆ 1) ê²€ìƒ‰ ì¡°ê±´ì— ê±¸ë¦° ì‚¬ëžŒì€ ${merged.length}ëª…, ê¸°ì¤€ì€ ${criteria?.join(
      ", "
    )}ì¸ë° ê° ì‚¬ëžŒì˜ ì •ë³´ë¥¼ ì½ì–´ë³´ì•˜ì„ ë•Œ, 2) ëª¨ë“  ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ì‚¬ëžŒì€ ${oneScoreCount}ëª…ìž…ë‹ˆë‹¤.
  1) ê²€ìƒ‰ ì¡°ê±´ì€ ê²€ìƒ‰ì— ë§žëŠ” SQL ì¿¼ë¦¬ë¥¼ LLMì´ ìƒì„±í•œ ë’¤ DBì— ì‚¬ìš©í–ˆì„ ë•Œ ì¶œë ¥ëœ ê²°ê³¼ë¡œ, ìµœëŒ€ 50ëª…ìœ¼ë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤. 2) ëª¨ë“  ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” ì‚¬ëžŒì€ ê° ì‚¬ëžŒë³„ë¡œ ë””í…Œì¼í•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¨ í›„, ì§ì ‘ assistantê°€ ì½ì–´ë³´ê³  íŒë‹¨í•œ ê²°ê³¼ìž…ë‹ˆë‹¤.
  ì´ ë•Œ, ìœ ì €ì—ê²Œ ì¶œë ¥í•  ì•ˆë‚´ë©”ì„¸ì§€ë¥¼ ì§§ê²Œ ìž‘ì„±í•˜ì„¸ìš”.

  ì •ë³´: SQL ì¿¼ë¦¬ëŠ” LLMì´ ìž‘ì„±í•˜ê¸° ë•Œë¬¸ì— í•´ë‹¹ ë¶€ë¶„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì„ ìˆ˜ë„ ìžˆë‹¤. limitì„ 50ìœ¼ë¡œ ê±¸ì—ˆê¸° ë•Œë¬¸ì— ê²€ìƒ‰ ì¡°ê±´ì— ê±¸ë¦°ê²Œ 50ëª…ì´ë¼ëŠ” ëœ»ì€, ì‹¤ì œë¡œ DBì—ëŠ” í•´ë‹¹ ì¿¼ë¦¬ë¥¼ ë§Œì¡±í•˜ëŠ”ê²Œ 50ëª… ì´ìƒì´ë¼ëŠ” ëœ»ì´ë‹¤. 
ìœ„ ì •ë³´ë¥¼ í•­ìƒ ìœ ì €ì—ê²Œ ë§í•˜ë¼ëŠ”ê±´ ì•„ë‹ˆê³ , í˜¹ì‹œ í•„ìš”í•˜ë©´ ì°¸ê³ í•´ë„ ë¨.
ì•„ëž˜ ê¸°ë³¸ ì¶œë ¥ ë©”ì„¸ì§€ëŠ” í•­ìƒ ìœ ì €ì—ê²Œ ë¦¬í„´ë˜ëŠ” ê°’ìœ¼ë¡œ, ì´ ë’¤ì— ì¶”ê°€ë¡œ ì¶œë ¥í•  ë‚´ìš©ë§Œ ìž‘ì„±í•˜ì„¸ìš”. ì ˆëŒ€ ê¸°ë³¸ ì¶œë ¥ ë©”ì„¸ì§€ë¥¼ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.

ê¸°ë³¸ ì¶œë ¥ ë©”ì„¸ì§€: ${defaultMsg}`,
    0.7,
    1
  );
  logger.log("msg ======================== ", msg);

  const res1 = await supabase.from("messages").insert({
    query_id: queryId,
    user_id: userId,
    role: 1,
    content: defaultMsg + "\n" + msg,
  });
  logger.log("res1 ", res1);

  // 7) Cache candidates for this page into runs_pages
  const candidatesForCache = merged.map((r: any) => ({
    score: r.score,
    id: r.id,
  }));

  const { error: insErr } = await supabase.from("runs_pages").insert({
    run_id: runId,
    page_idx: page,
    candidate_ids: candidatesForCache,
  });

  if (insErr) logger.log("runs_pages insert error:", insErr);

  const candidateIds = candidatesForCache.slice(0, 10).map((r: any) => r.id);

  if (
    page === 0 &&
    (candidateIds.length === 0 ||
      candidateIds.length < 10 ||
      candidateIds.length >= 50 ||
      oneScoreCount <= 5)
  ) {
    // const message = await makeMessage(
    //   query_text,
    //   criteria?.join(", ") ?? "",
    //   candidateIds.length === 0
    //     ? "no"
    //     : candidateIds.length < 10
    //     ? "less"
    //     : "more"
    // );
  }

  // 9) Slack notify if nothing found on first page
  // if (page === 0 && candidateIds.length === 0) {
  //   await notifyToSlack(
  //     `ðŸ” *Search Result Not Found! ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ìš”!*\n\n` +
  //       `â€¢ *Query*: ${query_text}\n` +
  //       `â€¢ *Criteria*: ${criteria?.join(", ")}\n` +
  //       `â€¢ *Run ID*: ${runId}\n` +
  //       `â€¢ *Time(Standard Korea Time)*: ${new Date().toLocaleString("ko-KR")}`
  //   );
  // }

  return NextResponse.json(
    { nextPageIdx, results: candidateIds, isNewSearch: true },
    { status: 200 }
  );
}
