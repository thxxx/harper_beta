import { geminiInference, xaiInference } from "@/lib/llm/llm";
import { supabase } from "@/lib/supabase";
import { buildSummary, ensureGroupBy, replaceName } from "@/utils/textprocess";
import { NextRequest, NextResponse } from "next/server";
import {
  criteriaPrompt,
  sqlPrompt2,
  sqlExistsPrompt,
} from "../../../lib/prompt";
import { generateSummary } from "./criteria_summarize/route";
import {
  deduplicateAndScore,
  mapWithConcurrency,
  ScoredCandidate,
  sumScore,
  updateRunStatus,
} from "./utils";
import { makeMessage } from "../hello/route";
import { ko } from "@/lang/ko";
import { notifyToSlack } from "@/lib/slack";
import { logger } from "@/utils/logger";

async function parseCriteria(
  queryText: string
): Promise<{ criteria: string[]; rephrasing: string; thinking: string }> {
  const prompt = `
${criteriaPrompt}
${queryText}
`.trim();
  // Responses API + structured outputs (text.format)
  const outText = await xaiInference(
    "grok-4-fast-reasoning",
    "You are a head hunting expert. Your input is a natural-language request describing criteria for searching job candidates.",
    prompt,
    0.5,
    1,
    false,
    "search_query_parser_harper_20260105"
  );

  const cleanedResponse = (outText as string).trim().replace(/\n/g, " ").trim();
  const outJson = JSON.parse(cleanedResponse);
  logger.log("outJson ", outJson);

  return outJson as any;
}

async function parseQueryWithLLM(
  queryText: string,
  criteria: string[],
  extraInfo: string = ""
): Promise<string | any> {
  try {
    let prompt = `
${sqlPrompt2}
Natural Language Query: ${queryText}
Criteria: ${criteria}
`.trim();
    if (extraInfo) {
      prompt += `
Extra Info: ${extraInfo}
`;
    }

    // Responses API + structured outputs (text.format)
    const outText = await geminiInference(
      "gemini-3-flash-preview",
      "You are a head hunting expertand SQL Query parser. Your input is a natural-language request describing criteria for searching job candidates.",
      prompt,
      0.5
    );
    const cleanText = (outText as string).trim().replace(/\n/g, " ").trim();

    // const transformedSqlQuery = transformSql(cleanedResponse);
    const sqlQuery = `
SELECT DISTINCT ON (T1.id)
  to_json(T1.id) AS id,
  T1.name,
  T1.headline,
  T1.location
FROM 
  candid AS T1
${cleanText}
`;
    const sqlQueryWithGroupBy = ensureGroupBy(sqlQuery, "");
    logger.log(
      "\n\n-------- ðŸ”¥ cleanedResponse1 ðŸ”¥ ---------\n\n",
      sqlQueryWithGroupBy,
      "\n\n-------- ðŸ”¥ cleanedResponse1 ðŸ”¥ ---------\n\n"
    );

    const pp2 =
      sqlExistsPrompt +
      `
Input SQL Query: 
"""
${sqlQueryWithGroupBy}
"""
`;
    await new Promise((resolve) => setTimeout(resolve, 2000));

    const outText2 = await geminiInference(
      "gemini-3-flash-preview",
      "You are a SQL Query refinement expert.",
      pp2,
      0.4
    );
    const cleanedResponse2 = (outText2 as string)
      .trim()
      .replace(/\n/g, " ")
      .trim();

    logger.log(
      "\n\n-------- â­ï¸ cleanedResponse2 â­ï¸ ---------\n\n",
      cleanedResponse2,
      "\n\n-------- â­ï¸ cleanedResponse2 â­ï¸ ---------\n\n"
    );

    // const outJson = JSON.parse(cleanedResponse2);
    const sqlQueryWithGroupBy2 = ensureGroupBy(cleanedResponse2, "");

    return sqlQueryWithGroupBy2;
  } catch (e) {
    console.error("parseQueryWithLLM error ", e);
    return e;
  }
}

/**
 * raw_input_text, criteriaë¥¼ ë°›ì•„ì„œ SQL ì¿¼ë¦¬ë¥¼ ë§Œë“¤ê³ , 50ëª…ì„ ê²€ìƒ‰í•˜ê³ , ìš”ì•½ì„ ë§Œë“¤ê³ , ë§Œì¡±í•˜ëŠ” 10ëª…ì„ ì ìˆ˜ì™€ í•¨ê»˜ ë¦¬í„´í•˜ëŠ” í•¨ìˆ˜
 */
const searchDatabase = async (
  raw_input_text: string,
  criteria: string[],
  pageIdx: number,
  queryId: string,
  userId: string,
  sql_query: string,
  limit: number = 50,
  offset: number = 0
) => {
  // const sqlQueryWithGroupBy = ensureGroupBy(sql_query, ""); // ë‹¤ë“¬ê¸°
  logger.log("sqlQueryWithGroupBy === \n", sql_query, "\n---\n");

  const upRes2 = await supabase.from("queries").upsert({
    query_id: queryId,
    user_id: userId,
    query: sql_query,
    status: ko.loading.searching_candidates,
  });

  const start_time = performance.now();
  let data: any[] | null = [];
  let error;
  const { data: data1, error: error1 } = await supabase.rpc(
    "set_timeout_and_execute_raw_sql",
    {
      sql_query: sql_query,
      page_idx: pageIdx,
      limit_num: limit,
      offset_num: offset,
    }
  );
  data = data1;
  error = error1;
  const end_time = performance.now();
  logger.log(
    "\n\ntime for fetching data : ",
    end_time - start_time,
    error,
    "\n\n"
  );

  if (error && error.message.includes("timeout")) {
    logger.log("\n\nâš ï¸ ê·¸ëƒ¥ Database ì¿¼ë¦¬ ìžì²´ë§Œ í•œë²ˆ ë” ì‹¤í–‰ ==");
    await updateRunStatus(queryId, ko.loading.searching_again);
    const { data: data2, error: error2 } = await supabase.rpc(
      "set_timeout_and_execute_raw_sql",
      {
        sql_query: sql_query,
        page_idx: pageIdx,
        limit_num: limit,
        offset_num: offset,
      }
    );
    data = data2;
    error = error2;
  }

  if (error) {
    logger.log("\n\nâš ï¸ First sql query error == try second == ", error);
    await updateRunStatus(queryId, ko.loading.retrying_error);

    let additional_prompt = "";
    if (error.message.includes("timeout")) {
      additional_prompt = `
If the error indicates a timeout (e.g., contains any of: timeout, statement timeout, canceling statement due to statement timeout, 504, Function timed out, deadline exceeded), treat it as a performance-fix task rather than a syntax-fix task.

**TIMEOUT rules**
- Preserve the meaning and returned rows as much as possible, but you MAY restructure the query ONLY to reduce execution time.
- Prefer a two-phase approach for speed: first select only T1.id (and any columns required for ORDER BY / DISTINCT ON) with restrictive filters and LIMIT, then join other tables to fetch the final columns.
- Do NOT add new tables. Do NOT add new filtering logic. Do NOT change ranking/ordering semantics.
- Allowed performance-only transformations (choose the minimum needed):
- Replace JOIN-based filtering with EXISTS subqueries when the joined table is used only for filtering (keeps semantics, reduces row explosion).
- If the query uses LEFT JOIN but filters on the joined table in WHERE, rewrite to EXISTS (or change to INNER JOIN) without changing logic.
- Add DISTINCT/DISTINCT ON only if the original query already implied deduplication or is returning duplicates due to joins (do not change results otherwise).
- Push down WHERE filters into the phase-1 id subquery/CTE so fewer rows are joined later.
- Avoid selecting large JSON/text columns in phase-1; fetch them only in phase-2.
- Keep all tsvector / tsquery logic as-is (@@ and tsquery functions), except minimal fixes required to avoid tsquery syntax errors.
- Output MUST be a single valid SQL statement only. No explanations.
- **ì¤‘ìš”** DB Search ì†ë„ë¥¼ ìœ„í•´ì„œëŠ” ë¨¼ì € ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” candidì˜ idë§Œ ë½‘ê³ , ê·¸ ë‹¤ìŒì— tableì„ JOINìœ¼ë¡œ ë¶™ì—¬ì•¼ í•œë‹¤.
`;
    }
    let fixed_query = await xaiInference(
      "grok-4-fast-reasoning",
      "You are a specialized SQL query fixing assistant. If there are any errors in the SQL query, fix them and return the fixed SQL query.",
      `You are an expert PostgreSQL SQL fixer for a recruitment candidate search system.

Goal:
Given (1) a PostgreSQL SQL query that failed and (2) the database error message, produce a corrected SQL query.

Critical rules:
- Fix ONLY what is necessary to resolve the SQL error.
- Do not change or fix the meaning of the query.
- Preserve the original intent and structure as much as possible (do not redesign the query).
- Do NOT add new tables, new joins, or new filters unless required to fix the error.
- Keep the tsvector search logic in place (tsvector column, to_tsquery/plainto_tsquery/websearch_to_tsquery, @@ operator).
- Keep the WHERE clause logic in place; only correct syntax/typing/aliasing/parentheses/quoting issues.
- If the error is caused by tsquery syntax, fix the query string minimally (escaping, removing illegal operators, using websearch_to_tsquery, etc.).
- If the error is caused by ambiguous columns/aliases, qualify with table aliases instead of changing logic.
- If the error is caused by type mismatch, cast minimally.
- Output MUST be a single valid SQL statement only. No explanations, no markdown, no comments, no codeblock.
- ì†ë„ë¥¼ ìœ„í•´ì„œëŠ” ë¨¼ì € idë§Œ ë½‘ê³ (LIMIT), ê·¸ ë‹¤ìŒì— tableì„ ë¶™ì´ëŠ”ê²Œ ë‚«ë‹¤.
${additional_prompt}
Inputs:
[SQL]
${sql_query},

[ERROR]
${error.message}

Return:
A corrected SQL query.
`,
      0.2,
      1
    );

    logger.log("âš ï¸ ==== fixed_query ==== \n\n", fixed_query);
    const sqlQueryWithGroupBy2 = ensureGroupBy(fixed_query as string, "");
    const upRes3 = await supabase.from("queries").upsert({
      query_id: queryId,
      user_id: userId,
      query: fixed_query as string,
      status: ko.loading.searching_candidates,
    });

    const { data: data2, error: error2 } = await supabase.rpc(
      "set_timeout_and_execute_raw_sql",
      {
        sql_query: sqlQueryWithGroupBy2,
        page_idx: pageIdx,
        limit_num: limit,
        offset_num: offset,
      }
    );
    data = data2;
    error = error2;
  }

  logger.log(
    "ì´ ê°€ì ¸ì˜¨ ì§€ì›ìž data : ",
    data?.[0]?.length,
    // JSON.stringify(data?.[0]?.slice(0, 1), null, 2),
    "\nError : ",
    error
  );
  if (!data || !data[0] || data[0].length === 0) {
    return [];
  }

  await updateRunStatus(queryId, ko.loading.summarizing);

  const fullScore = criteria.length * 2;
  // 1. LLM ìš”ì•½ ë° ì ìˆ˜ ê³„ì‚°ë§Œ ë¨¼ì € ìˆ˜í–‰
  const scored: (ScoredCandidate & { summary: string })[] =
    await mapWithConcurrency(data[0] as any[], 17, async (candidate) => {
      const id = candidate.id as string;
      let summary: string | null = null;
      let lines: string[] = [];

      try {
        summary = (await generateSummary(
          candidate,
          criteria,
          raw_input_text
        )) as string;
        lines = JSON.parse(summary);
      } catch (e) {
        lines = [];
        summary = "";
      }

      const score = sumScore(lines);

      return {
        id,
        score:
          fullScore !== 0 ? Math.round((score / fullScore) * 100) / 100 : 1,
        summary, // ë‚˜ì¤‘ì— ì €ìž¥í•˜ê¸° ìœ„í•´ ê²°ê³¼ì— í¬í•¨
      };
    });

  // 2. DBì— ì €ìž¥í•  ë°ì´í„° í•„í„°ë§ (ì—ëŸ¬ ë“±ìœ¼ë¡œ ìš”ì•½ì´ ì—†ëŠ” ê²½ìš° ì œì™¸)
  const upsertData = scored
    .filter((s) => s.summary !== null)
    .map((s) => ({
      candid_id: s.id,
      query_id: queryId,
      text: s.summary,
    }));

  // 3. í•œ ë²ˆì˜ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ìœ¼ë¡œ ëª¨ë‘ ì €ìž¥ (Batch Upsert)
  if (upsertData.length > 0) {
    const { error } = await supabase
      .from("synthesized_summary")
      .upsert(upsertData); // ë‹¨ì¼ ìš”ì²­ìœ¼ë¡œ Nê°œ ì €ìž¥

    if (error) console.error("Batch upsert error:", error);
  }

  // Sort desc by score, tie-breaker optional (stable-ish by id)
  scored
    .sort((a, b) => b.score - a.score || a.id.localeCompare(b.id))
    .map((s) => ({ score: s.score, id: s.id }));

  return scored;
};

export async function POST(req: NextRequest) {
  const body = await req.json();
  const { queryId, pageIdx } = body;

  if (!queryId)
    return NextResponse.json({ error: "Missing queryId" }, { status: 400 });

  let offset = 0;
  let cachedCandidates: any[] = [];

  // ì´ë¯¸ ê²€ìƒ‰í•´ë‘” ê²°ê³¼ê°€ ìžˆëŠ”ì§€ ì°¾ê¸°
  const { data: resultsPages, error: lpErr } = await supabase
    .from("query_pages")
    .select("*")
    .eq("query_id", queryId)
    .eq("page_idx", pageIdx)
    .order("created_at", { ascending: false });

  const nextPageIdx = pageIdx + 1;
  const cachedResults = resultsPages?.[0];

  logger.log(pageIdx, "ì¿¼ë¦¬ì™€ results ", cachedResults);

  // ì´ë¯¸ ê²€ìƒ‰í•œ ê²°ê³¼ê°€ ìžˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ë¦¬í„´
  if (cachedResults && cachedResults.candidate_ids) {
    const candidateIds = cachedResults.candidate_ids
      .slice(0, 10)
      .map((r: any) => r.id);
    return NextResponse.json(
      { nextPageIdx, results: candidateIds },
      { status: 200 }
    );
  } else if (pageIdx > 0) {
    const { data: prevResultsPages } = await supabase
      .from("query_pages")
      .select("*")
      .eq("query_id", queryId)
      .eq("page_idx", pageIdx - 1)
      .order("created_at", { ascending: false });

    const prevCachedResults = prevResultsPages?.[0];
    if (
      !prevResultsPages ||
      !prevCachedResults ||
      !prevCachedResults.candidate_ids ||
      prevCachedResults.candidate_ids.length === 0
    ) {
      return NextResponse.json({ nextPageIdx, results: [] }, { status: 200 });
    }
    const isLoadMore =
      (prevCachedResults.candidate_ids.length + pageIdx * 10) % 50 === 0;
    logger.log("prevCachedResults ", prevCachedResults.candidate_ids?.length);
    if (!isLoadMore) {
      logger.log("\n\n50ì˜ ë°°ìˆ˜ê°€ ì•„ë‹Œ ê²½ìš° ê·¸ëƒ¥ 10ê°œ ë¦¬í„´\n\n");
      // ì ìˆ˜ ìˆœìœ¼ë¡œ ë‚˜ì—´, ì•ž 10ê°œ ì œì™¸í•˜ê³  ë’¤ (N-10)ê°œ ì €ìž¥, ë¦¬í„´ì€ max(N-10, 10)ê°œ ë¦¬í„´
      // ê°™ì€ ì¿¼ë¦¬ë¥¼ ë‚ ë ¤ë´¤ìž 50ëª… ì´í•˜ì´ê¸° ë•Œë¬¸ì— ë˜‘ê°™ìŒ.
      const candidateIds = prevCachedResults.candidate_ids.slice(10);
      await supabase.from("query_pages").insert({
        message_id: 0,
        query_id: queryId,
        page_idx: pageIdx,
        candidate_ids: candidateIds,
      });
      return NextResponse.json(
        {
          nextPageIdx,
          results: candidateIds.slice(0, 10).map((r: any) => r.id),
        },
        { status: 200 }
      );
    } else if (isLoadMore) {
      logger.log("\n\n50ì˜ ë°°ìˆ˜ì¸ ê²½ìš°\n\n");
      const candidateIds = prevCachedResults.candidate_ids.slice(10);
      const scoreSum = candidateIds
        .slice(0, 10)
        .reduce((acc: number, curr: any) => acc + curr.score, 0);
      if (scoreSum >= 10) {
        await supabase.from("query_pages").insert({
          message_id: 0,
          query_id: queryId,
          page_idx: pageIdx,
          candidate_ids: candidateIds,
        });
        return NextResponse.json(
          {
            nextPageIdx,
            results: candidateIds.slice(0, 10).map((r: any) => r.id),
          },
          { status: 200 }
        );
      } else {
        //
        offset = 50;
        cachedCandidates = candidateIds;
      }
    }
  }

  // ì €ìž¥ë˜ì–´ìžˆëŠ” ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ìƒˆë¡­ê²Œ ê²€ìƒ‰í•´ì•¼í•œë‹¤ëŠ” ëœ».
  const { data: q, error: qErr } = await supabase
    .from("queries")
    .select("query_id,user_id,raw_input_text,query,criteria")
    .eq("query_id", queryId)
    .single();

  logger.log("ì¼ë‹¨ ì¿¼ë¦¬ í™•ì¸ : ", q);

  if (qErr || !q || !q.raw_input_text)
    return NextResponse.json({ error: "Query not found" }, { status: 404 });

  const uploadBestTenCandidates = async (fullCandidates: any[]) => {
    await updateRunStatus(
      queryId,
      "Got Best 10 Candidates. Now organizing results."
    );
    const candidates = fullCandidates.map((r: any) => ({
      score: r.score,
      id: r.id,
    }));
    // const candidateIds = fullCandidates.slice(0, 10).map((r: any) => r.id);
    const { error: insErr } = await supabase.from("query_pages").insert({
      query_id: queryId,
      page_idx: pageIdx,
      candidate_ids: candidates,
      message_id: 0,
    });

    return candidates.slice(0, 10).map((r: any) => r.id);
  };

  // input queryë¡œ SQLë¬¸ì„ ë§Œë“¤ì–´ë’€ëŠ”ì§€ ì•„ë‹Œì§€
  // ì €ìž¥ë˜ì–´ìžˆëŠ” ê²°ê³¼ê°€ ì—†ë‹¤ë©´ ìƒˆë¡­ê²Œ ê²€ìƒ‰í•´ì•¼í•œë‹¤ëŠ” ëœ».
  let parsed_query = q.query;
  let criteria = q.criteria;

  if (!parsed_query) {
    await updateRunStatus(queryId, ko.loading.making_criteria);
    const {
      criteria: criteria1,
      rephrasing,
      thinking,
    } = await parseCriteria(q.raw_input_text);
    criteria = criteria1;

    const upRes = await supabase.from("queries").upsert({
      query_id: queryId,
      user_id: q.user_id,
      criteria: criteria,
      thinking: rephrasing + " | " + thinking,
      status: ko.loading.making_query,
    });

    parsed_query = await parseQueryWithLLM(q.raw_input_text, criteria, "");
    if (typeof parsed_query !== "string") {
      await updateRunStatus(queryId, JSON.stringify(parsed_query));
      return NextResponse.json(parsed_query, { status: 404 });
    }
  }
  // ì¿¼ë¦¬ë¥¼ ë§Œë“¤ì—ˆë‹¤.
  let searchResults = await searchDatabase(
    q.raw_input_text ?? "",
    criteria ?? [],
    pageIdx,
    queryId,
    q.user_id,
    parsed_query,
    50,
    offset
  );
  logger.log(`idWithScores === ${searchResults.length} nums `, searchResults);

  // scoreê°€ 1ì ì¸ ì‚¬ëžŒ ìˆ˜
  const oneScoreCount = searchResults.filter((r: any) => r.score === 1).length;
  logger.log(searchResults.length, " oneScoreCount === ", oneScoreCount);

  const mergeCachedCandidates = deduplicateAndScore(
    searchResults,
    cachedCandidates
  );
  logger.log("mergeCachedCandidates ", mergeCachedCandidates.length);
  const candidateIds = await uploadBestTenCandidates(mergeCachedCandidates);

  if (
    pageIdx === 0 &&
    (candidateIds.length === 0 ||
      candidateIds.length < 10 ||
      candidateIds.length >= 50 ||
      oneScoreCount <= 5)
  ) {
    const message = await makeMessage(
      q.raw_input_text ?? "",
      criteria?.join(", ") ?? "",
      candidateIds.length === 0
        ? "no"
        : candidateIds.length < 10
        ? "less"
        : "more"
    );
    logger.log("message ", message);
    if (message) {
      logger.log("ë“¤ì–´ëŠ” ì˜µë‹ˆë‹¤. message ", message["message"]);
      const res = await supabase.from("queries").upsert({
        query_id: queryId,
        user_id: q.user_id,
        message: message["message"],
        recommendation: message["recommendations"]?.join("|") ?? "no",
      });
      logger.log("res ", res);
    }
  }
  if (pageIdx === 0 && candidateIds.length === 0) {
    await notifyToSlack(`ðŸ” *Search Result Not Found! ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ìš”!*

â€¢ *Query*: ${q.raw_input_text}
â€¢ *Criteria*: ${criteria?.join(", ")}
- *User ID*: ${q.user_id}
â€¢ *Time(Standard Korea Time)*: ${new Date().toLocaleString("ko-KR")}`);
  }
  return NextResponse.json(
    { nextPageIdx, results: candidateIds, isNewSearch: true },
    { status: 200 }
  );
}
