import { OpenAI } from "openai";
import { logger } from "@/utils/logger";

if (typeof window !== "undefined") {
  throw new Error("llm.ts was bundled into the client!");
}

export enum OpenAIResponse {
  DELTA = "response.output_text.delta",
  DONE = "response.content_part.done",
}

export type Message = {
  role: "system" | "user" | "assistant";
  content: string;
};

export const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  dangerouslyAllowBrowser: true,
});

export const xaiClient = new OpenAI({
  apiKey: process.env.GROK_API_KEY,
  dangerouslyAllowBrowser: true,
  baseURL: "https://api.x.ai/v1",
});

export type OnToken = (token: string) => void;

const pricingTable = {
  "grok-4-fast-reasoning": {
    input: 0.2 / 1_000_000,
    output: 0.5 / 1_000_000,
  },
  "grok-4-fast-non-reasoning": {
    input: 0.2 / 1_000_000,
    output: 0.5 / 1_000_000,
  },
  "gpt-5-mini": {
    input: 0.25 / 1_000_000,
    output: 2 / 1_000_000,
  },
  "gemini-3-flash-preview": {
    input: 0.5 / 1_000_000,
    output: 3 / 1_000_000,
  },
};

export const xaiInference = async (
  model: "grok-4-fast-reasoning" | "grok-4-fast-non-reasoning" | "gpt-5-mini",
  systemPrompt: string,
  userPrompt: string,
  temperature: number = 0.7,
  max_retries: number = 1,
  is_json: boolean = false,
  prompt_cache_key: string = ""
): Promise<string> => {
  const response = await xaiClient.chat.completions.create({
    model: model,
    messages: [
      { role: "system", content: systemPrompt },
      { role: "user", content: userPrompt },
    ],
    temperature: temperature,
    prompt_cache_key: prompt_cache_key,
  });

  const content = response.choices[0]?.message?.content;

  const usage = response.usage ?? {
    prompt_tokens: 0,
    completion_tokens: 0,
    total_tokens: 0,
  };

  // grok-4-fast pricing (USD)
  const inputTokenPrice = pricingTable[model].input;
  const outputTokenPrice = pricingTable[model].output;

  const cost =
    usage.prompt_tokens * inputTokenPrice +
    usage.completion_tokens * outputTokenPrice +
    (usage.completion_tokens_details?.reasoning_tokens ?? 0) * outputTokenPrice;

  // logger.log("cost ", cost * 1450, "ì›");

  return content ?? "";
};

import { GoogleGenAI, ThinkingLevel } from "@google/genai";
import { supabase } from "../supabase";
const gemini = new GoogleGenAI({
  apiKey: process.env.GEMINI_API_KEY,
});

export async function openaiInference({
  model,
  systemPrompt,
  userPrompt,
  temperature
}: {
  model: "gpt-4.1-mini" | "gpt-4.1-nano" | "gpt-5-mini" | "gpt-5.2";
  systemPrompt: string;
  userPrompt: string;
  temperature: number;
}): Promise<string> {
  console.log("openaiInference", model, temperature);

  const response = await client.chat.completions.create({
    model: model,
    messages: [
      { role: "system", content: systemPrompt },
      { role: "user", content: userPrompt },
    ],
    temperature: temperature,
  });

  return response?.choices?.[0]?.message?.content ?? "";
}

export async function geminiInference(
  model: "gemini-3-flash-preview",
  systemPrompt: string,
  userPrompt: string,
  temperature: number = 0.7,
  thinkingLevel: ThinkingLevel = ThinkingLevel.LOW
): Promise<string | object> {
  try {
    const response = await gemini.models.generateContent({
      model: model,
      contents: systemPrompt + "\n\n" + userPrompt,
      config: {
        thinkingConfig: {
          thinkingLevel: thinkingLevel,
        },
        temperature: temperature,
      },
    });
    // logger.log("response ", response);
    // logger.log("response ", response.usageMetadata?.promptTokensDetails);

    const cost =
      (response.usageMetadata?.promptTokenCount ?? 0) *
        pricingTable[model].input +
      (response.usageMetadata?.candidatesTokenCount ?? 0) *
        pricingTable[model].output;

    logger.log("[GEMINI] cost ", cost * 1450, "ì›");

    const text = response?.text?.trim()?.replace(/^```\w*\s*/, "")?.replace(/\s*```$/, "")?.trim() ?? "";
    if(text.length === 0) {
      throw new Error("Gemini inference returned empty text");
    }
    return text;
  } catch (e) {
    logger.log("ğŸš¨ geminiInference error:", e, "\nSo, use xaiInference instead.");
    // await supabase.from("landing_logs").insert({
    //   type: JSON.stringify(e),
    // });
    // const response = await xaiInference("grok-4-fast-reasoning", systemPrompt, userPrompt, temperature, 1, false, "geminiInferenceError");
    // return response;

    const response  = await openaiInference({
      model: "gpt-5.2",
      systemPrompt: systemPrompt,
      userPrompt: userPrompt,
      temperature: temperature,
    });
    return response;
  }
}

const inference = async (
  model: "gpt-4.1-nano" | "gpt-4.1-mini",
  systemPrompt: string,
  userPrompt: string
): Promise<string> => {
  const response = await client.chat.completions.create({
    model: model,
    messages: [
      { role: "system", content: systemPrompt },
      { role: "user", content: userPrompt },
    ],
  });
  logger.log("response ", response.usage);

  const content = response.choices[0]?.message?.content;
  return content ?? "";
};

export const makeQuestion = async (
  conversationHistory: string,
  userInfo: string,
  resumeText: string
): Promise<string> => {
  logger.log("conversationHistory", conversationHistory);
  logger.log("userInfo", userInfo);
  logger.log("resumeText", resumeText);

  const userPrompt = `
í—¤ë“œí—Œí„°ë¼ê³  ìƒê°í•˜ê³ , ì´ ì‚¬ëŒì´ ì›í•˜ëŠ” ê²ƒ, ì–´ë–¤ íšŒì‚¬ë‘ ë§¤ì¹­ì‹œì¼œì£¼ë©´ ì¢‹ì„ì§€ë¥¼ ì•Œì•„ë‚´ê¸° ìœ„í•´ ëŒ€í™”ë¥¼ í•˜ê³ ìˆì–´. ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ê³ ë ¤í•´ì„œ í•„ìš”í•œ ì¶”ê°€ì§ˆë¬¸ì´ ìˆìœ¼ë©´ í•´ë„ ë¼. ì—†ìœ¼ë©´ ì¼ë‹¨ ì´ ìˆœì„œë¥¼ ë”°ë¼í•´ë„ ë¼.

ê·¸ë¦¬ê³  ë§Œì•½ ë‹¤ìŒì— í•´ì•¼í•˜ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ ì´ì „ì— ë‹µë³€ìœ¼ë¡œ ë‚˜ì˜¨ ì ì´ ìˆìœ¼ë©´ ì§ˆë¬¸ì„ ìƒëµí•˜ê³  ê±´ë„ˆ ë›°ì–´ë„ ë¼.

ë„ˆë¬´ ì§ˆë¬¸ë§Œ ë˜ì§€ì§€ ë§ê³ , ì´ì „ì— ìœ ì €ì˜ ë‹µë³€ì´ ê¸¸ì—ˆë‹¤ë©´ í•œë²ˆ ì¬ìš”ì•½í•˜ê³  ì§ˆë¬¸ì„ ë˜ì§€ë˜ê°€, ë„¤ ì•Œê² ìŠµë‹ˆë‹¤. ê°™ì€ ë§ë¡œ ì‹œì‘í•´ì¤˜.

---
ì´ê±´ ì§„í–‰ë˜ì–´ì•¼í•˜ëŠ” ë¦¬í¬ë£¨íŒ…/í—¤ë“œí—ŒíŒ… Call ì˜ˆì‹œ í”„ë¡œì„¸ìŠ¤ì•¼.

1. [ì§€ì›ìì˜ í˜„ì¬ ìƒí™© íŒŒì•…]
    1. í˜„ì¬ next stepì— ëŒ€í•´ì„œ ì–´ë–»ê²Œ ìƒê°í•˜ê³  ê³„ì‹œë‚˜ìš”?
        
        [ì´ì§ì´ë¼ë©´]
        
        1. ì™œ ì´ì§ì„ ìƒê°í•˜ê³  ê³„ì‹ ê°€ìš”? í˜„ ì§ì¥ì—ì„œ ì•„ì‰¬ìš´ ì  í˜¹ì€ ì•ìœ¼ë¡œ ì›í•˜ëŠ” ì 
        2. ë‹¹ì¥ ìµœëŒ€í•œ ë¹¨ë¦¬ ì¸ê°€ìš” ì˜µì…˜ ì—´ì–´ë‘ê¸° ìˆ˜ì¤€ì¸ê°€ìš”?
    2. ì§€ê¸ˆì€ ì–´ë–¤ê±¸ í•˜ê³  ê³„ì‹œë‚˜ìš”?
2. [ì»¤ë¦¬ì–´ ë°©í–¥ì„± / í¬ë§ ì—­í• ]
    1. ë‹¤ìŒ íšŒì‚¬ë¥¼ ì°¾ì„ ë•Œ ì–´ë–¤ íŒ€ì—ì„œ ì¼í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?
        1. [ëŒ€ë‹µì´ ë¶ˆì™„ì „ í•˜ë‹¤ë©´] ìŠ¤íƒ€íŠ¸ì—… vs ëŒ€ê¸°ì—…
    2. ë³¸ì¸ì˜ ì¼í•˜ëŠ” ìŠ¤íƒ€ì¼ì´ ìˆë‚˜ìš”?
    3. ì›í•˜ëŠ” íŒ€ ë¬¸í™”ê°€ ìˆìœ¼ì„¸ìš”? (ë³´ìƒ / íŒ€ ìˆ˜ì¤€ / ì„±ì¥ì„± / ë¹„ì „ / ì›Œë¼ë°¸ / ê¸°ìˆ  ìŠ¤íƒ / ë¦¬ëª¨íŠ¸ ë“±)
    4. ì•ìœ¼ë¡œ 3~5ë…„ë‚´ ì»¤ë¦¬ì–´ ëª©í‘œê°€ ì–´ë–»ê²Œ ë˜ì‹œë‚˜ìš”?
    5. ë‹¤ìŒ ì»¤ë¦¬ì–´ ìŠ¤í…ì—ì„œëŠ” ì–´ë–¤ ì—­í• ì„ ë§¡ê³  ì‹¶ìœ¼ì„¸ìš”?
3. [ì „ë¬¸ ë¶„ì•¼ / ê°•ì  / ]
    1. ì œì•ˆì´ ë“¤ì–´ì˜¨ë‹¤ë©´ ë°›ì•„ë³¼ ì˜ì‚¬ê°€ ìˆëŠ” ì—­í• ë“¤ì„ ì•„ë˜ì—ì„œ ê³¨ë¼ì£¼ì„¸ìš”.
    2. ì´ë ¥ì„œë¥¼ ë³´ë‹ˆ ~~ê°€ ìˆëŠ”ë°, ì–´ë–¤ ì—­í• ì„ í–ˆê³ , ì–´ë–¤ê±¸ ë°°ì› ëŠ”ì§€ ë§í•´ì£¼ì„¸ìš”.
        1. ë³¸ì¸ì´ ë§í•˜ê³  ì‹¶ì€ ì¤‘ìš”í–ˆë˜ í”„ë¡œì íŠ¸ë‚˜ ê²½í—˜ ìˆìœ¼ì„¸ìš”?
    3. íŒ€ì—ì„œ ì–´ë–¤ ì—­í• ë¡œ ê°€ì¥ ê°•ì ì„ ë°œíœ˜í•œë‹¤ê³  ìƒê°í•˜ë‚˜ìš”?
4. [ë§ˆì§€ë§‰ ë³´ìƒ ë“±]
    1. í•´ì™¸ ê·¼ë¬´ë‚˜ ì¬ë°°ì¹˜ ê°€ëŠ¥í•˜ì‹ ê°€ìš”?
    2. í¬ë§ ì—°ë´‰ì€ ì–´ëŠ ì •ë„ì¸ê°€ìš”?
    3. ê·¼ë¬´ ê°€ëŠ¥í•œ ì‹œì ì´ ì–¸ì œì¸ê°€ìš”?
    4. ì¶”ê°€ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì , ë§í•˜ê³  ì‹¶ì€ê²Œ ìˆìœ¼ì‹ ê°€ìš”?
    5. í”¼ë“œë°±
---

---

Example Outputs

â€˜ê¹€í˜¸ì§„ë‹˜, ì•ˆë…•í•˜ì„¸ìš”. OptimizerAIì—ì„œ ê³µë™ ì°½ì—…ì ê²¸ ì—°êµ¬ ì—”ì§€ë‹ˆì–´ë¡œ ê³„ì…¨ê³ , ì´ì „ì—ëŠ” ëµìŠ¤í”Œë¡œìš°ì—ì„œ ML Scientistë¡œ ê³„ì‹œë©´ì„œ ì¸ìƒì ì¸ ì„±ê³¼ë¥¼ ë‚´ì…¨ìŠµë‹ˆë‹¤. ë¨¼ì € í˜„ì¬ **ë‹¤ìŒ ì»¤ë¦¬ì–´ ìŠ¤í…ì— ëŒ€í•´ì„œ ì–´ë–»ê²Œ ìƒê°í•˜ê³  ê³„ì‹ ì§€** ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?â€™

â€˜OptimizerAIë¥¼ ë‚˜ì˜¤ì‹  ì´ìœ ë„ ì˜ ì´í•´í–ˆìŠµë‹ˆë‹¤. ë³¸ì¸ì´ ì›í•˜ëŠ” ë°©í–¥ê³¼ íšŒì‚¬ì˜ ì „ëµì´ ì¡°ê¸ˆ ë‹¬ë¼ì§„ ëŠë‚Œì´ì…¨ë˜ ê²ƒ ê°™ë„¤ìš”. ê·¸ë ‡ë‹¤ë©´ ë‹¤ìŒ íšŒì‚¬ì—ì„œ ì ˆëŒ€ ë¹ ì§€ë©´ ì•ˆ ëœ ìš”ì†Œ, ì˜ˆë¥¼ ë“¤ì–´ íŒ€ ìˆ˜ì¤€, ë¹„ì „, ê¸°ìˆ  ìŠ¤íƒ, ë³´ìƒ, ë¦¬ëª¨íŠ¸ ì—¬ë¶€ ë“±ì´ ìˆì„ê¹Œìš”?â€™

â€˜ì§€ê¸ˆê¹Œì§€ ë§ì”€í•´ì£¼ì‹  ê±¸ ë³´ë©´, íŠ¹íˆ ëŒ€ê·œëª¨ ì˜¤ë””ì˜¤ ëª¨ë¸ ì—°êµ¬ì™€ ì‹¤ì œ í”„ë¡œë•íŠ¸ ëŸ°ì¹­ê¹Œì§€ì˜ ê²½í—˜ì´ ê¹€í˜¸ì§„ë‹˜ ì»¤ë¦¬ì–´ì˜ í° ì¶•ì´ ëœ ê²ƒ ê°™ì•„ìš”. ê¸°ìˆ ì  ê¹Šì´ë„ ìˆì§€ë§Œ, ìœ ì € ì¸í„°ë·°ë¶€í„° ê¸°ëŠ¥ ê¸°íšê¹Œì§€ ì§ì ‘ í•˜ì…¨ë˜ ì ë„ ì¸ìƒì ì´ë„¤ìš”. ê·¸ë ‡ë‹¤ë©´ ë‹¤ìŒ íšŒì‚¬ì—ì„œëŠ” ì—°êµ¬ ì¤‘ì‹¬ì˜ ì—­í• ê³¼ ì œí’ˆ ì¤‘ì‹¬ì˜ ì—­í•  ì¤‘ ì–´ëŠ ìª½ì„ ë” ì„ í˜¸í•˜ì‹œë‚˜ìš”? ìƒê´€ì—†ë‹¤ë©´ ë‘˜ë‹¤ ìƒê´€ì—†ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”.â€™

â€˜Co-founderë¡œì„œ ìŠ¤íƒ€íŠ¸ì—…ì˜ ì„±ì¥ì„ ì§ì ‘ ì´ëŒì–´ ë³´ì…¨ê³ , ë”¥í…Œí¬ ì¸ë ¥ë“¤ê³¼ êµë¥˜í•˜ë©° ì‚°ì—…ë‹¨ì˜ ì¤‘ìš”í•œ ì ì„ ë°°ìš°ì…¨ë‹¤ê³  ë“¤ì—ˆìŠµë‹ˆë‹¤. ê¹€í˜¸ì§„ë‹˜ê»˜ì„œ **ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” íŒ€ ë¬¸í™”ë‚˜ í™˜ê²½ì ì¸ ìš”ì†Œ (ì˜ˆ: ë³´ìƒ, ì„±ì¥ì„±, ê¸°ìˆ  ìŠ¤íƒ, ì›Œë¼ë°¸ ë“±)** ê°€ ìˆë‹¤ë©´ ë¬´ì—‡ì¼ê¹Œìš”?â€™

---

ì•„ë˜ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ í†µí™” ê¸°ë¡, ì§€ì›ì ì •ë³´, ê·¸ë¦¬ê³  ì§€ì›ìì˜ ì´ë ¥ì„œ ì „ë¬¸ì…ë‹ˆë‹¤.

ì´ ì„¸ ê°€ì§€ë¥¼ ì¢…í•©í•´ì„œ, ë‹¤ìŒ í„´ì— Harper(ë‹¹ì‹ )ê°€ ì§€ì›ìì—ê²Œ ë¬¼ì–´ë³¼ â€œê°€ì¥ ìì—°ìŠ¤ëŸ½ê³  ì ì ˆí•œ ì§ˆë¬¸â€ì„  í¬í•¨í•œ ë‹¤ìŒ ë°œí™”ë¥¼ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.

- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë§í•˜ì„¸ìš”.
- ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë³µì¡í•˜ì§€ ì•Šê²Œ ìì—°ìŠ¤ëŸ¬ìš´ ë©´ì ‘ ì§ˆë¬¸ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
- í•œ ë²ˆì— ì§ˆë¬¸ í•˜ë‚˜ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
- ì§€ì›ìì˜ ê´€ì‹¬ ë¶„ì•¼, ê²½ë ¥, ëª©í‘œ ë“±ì— ê°€ì¥ ë§ëŠ” ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”.
- ì´ì „ ì§ˆë¬¸ê³¼ ì¤‘ë³µë˜ê±°ë‚˜ ë„ˆë¬´ ë¹„ìŠ·í•œ ë‚´ìš©ì€ í”¼í•˜ì„¸ìš”.

ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•œë‹¤ë©´ ë‹µë³€í•œ ë’¤ ê¼­ ë§ˆì§€ë§‰ì—” ë‹¤ìŒ ì§ˆë¬¸ì„ í•´ì•¼í•˜ê³ , ë§Œì•½ ë¦¬í¬ë£¨íŒ…/í—¤ë“œí—ŒíŒ…ê³¼ ë¬´ê´€í•œ ì§ˆë¬¸ì´ë‚˜ ìš”êµ¬ë¥¼ í•˜ë©´ ì €ëŠ” ì§€ì›ìë‹˜ì˜ ì»¤ë¦¬ì–´ë¥¼ ìœ„í•œ agentì´ê¸° ë•Œë¬¸ì— ë‹¤ë¥¸ê±´ í•  ìˆ˜ ì—†ë‹¤ê³  ëŒ€ë‹µí•´.

----
[ì´ì „ ëŒ€í™” ê¸°ë¡]
${conversationHistory}

[ì§€ì›ì ì •ë³´]
${userInfo}

[ì§€ì›ì ì´ë ¥ì„œ]
${resumeText}
----
`;

  return await inference(
    "gpt-4.1-mini",
    "You are a helpful assistant. Your name is Harper, voice-based AI recruiter for user. Use korean language. But it's okay to use english words if it is specific terms.",
    userPrompt
  );
};

export const callGreeting = async (
  userInfo: string,
  resumeText: string
): Promise<string> => {
  // return await inference(
  //   "gpt-4.1-nano",
  //   "You are a helpful assistant. Your name is Harper, voice-based AI recruiter for user. Use only korean language.",
  //   "ì§€ê¸ˆ ìœ ì €ê°€ ìƒˆë¡­ê²Œ í†µí™”ì„¸ ì°¸ì—¬í–ˆì–´."
  // );
  // return "í•˜í¼ì…ë‹ˆë‹¤. ì €í¬ëŠ” ëª¨ë“  ì§€ì›ì ë¶„ë“¤ì„ í•œë¶„í•œë¶„ ì •ì„±ìŠ¤ëŸ½ê²Œ ê´€ë¦¬í•©ë‹ˆë‹¤. ì´ì œ ì§§ê²Œ í†µí™”ê°€ ì‹œì‘ë  ì˜ˆì •ì¸ë°, ê°€ëŠ¥í•˜ë©´ ì¡°ìš©í•œ ê³³ì—ì„œ ì§„í–‰í•´ ì£¼ì„¸ìš”. ì˜¤ë””ì˜¤ë‚˜ ë§ˆì´í¬ëŠ” ê´œì°®ìœ¼ì‹ ê°€ìš”?";
  return "í•˜í¼ì…ë‹ˆë‹¤. ì˜¤ë””ì˜¤ë‚˜ ë§ˆì´í¬ëŠ” ê´œì°®ìœ¼ì‹ ê°€ìš”?";
};

export function parseResumeJson(jsonString: string) {
  try {
    const data = JSON.parse(jsonString);

    return {
      education: Array.isArray(data.education) ? data.education : [],
      workExperiences: Array.isArray(data.workExperiences)
        ? data.workExperiences
        : [],
    };
  } catch (e) {
    console.error("Failed to parse JSON:", e);
    return { education: [], workExperiences: [] };
  }
}

// í•œ ë²ˆ í˜¸ì¶œí•´ì„œ ë¬¸ìì—´ë¡œ ë‹µë§Œ ë°›ì•„ì˜¤ëŠ” í—¬í¼
export async function extractResumeInfo(resumeText: string): Promise<any> {
  const response = await client.chat.completions.create({
    model: "gpt-4.1-mini",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      {
        role: "user",
        content: `
You are an information extraction system.  
Your task is to read the resume text provided by the user and extract two things:

1. education: Education[]
2. workExperiences: WorkExperience[]

âš ï¸ OUTPUT RULES (IMPORTANT)
- Output must be valid JSON ONLY. No explanations.
- The root structure must be exactly:
  {
    "education": [...],
    "workExperiences": [...]
  }
- Even if information is missing or unclear, you MUST output arrays.
- Missing or ambiguous fields should be an empty string "".
- If you are not sure about dates, return "".
- If a person is currently studying or working, set endDate: "default".

ìš°ì„  education, workExperiencesë¥¼ ê°ê° ê°€ì¥ ìµœê·¼ ê°’ ê¸°ì¤€ìœ¼ë¡œ í•˜ë‚˜ì”©ë§Œ ì¶œë ¥í•´ì¤˜.
educationì€ ì „ë¶€ í•œê¸€ë¡œ, workExperiencesëŠ” ì „ë¶€ ì˜ì–´ë¡œ ì¶œë ¥í•´ì¤˜.

------------------------------------
DATA FORMAT

Education = {
  "school": string, // í•™êµëª…
  "major": string,  // ì „ê³µ
  "startDate": string,    // ì…í•™ì¼ (unknown â†’ "")
  "endDate": string,      // ì¡¸ì—…ì¼ (ì¬í•™ì¤‘ â†’ "default")
  "degree": string, // í•™ì‚¬/ì„ì‚¬/ë°•ì‚¬
  "gpa": string // í•™ì  (N/4.3)
}

WorkExperience = {
  "company": string, // íšŒì‚¬ëª…
  "position": string, // ì§ë¬´
  "startDate": string,
  "endDate": string,      // ì¬ì§ì¤‘ì´ë©´ "default"
  "description": string // ì„¤ëª…, 3ì¤„ ìš”ì•½
}

------------------------------------
RESUME TEXT:
${resumeText}
------------------------------------

Now extract all information and output JSON only. Do not include \`\`\`json or \`\`\` at the beginning or end.
`,
      },
    ],
  });

  const content = response.choices[0]?.message?.content;
  logger.log("extractResumeInfo", content);
  return parseResumeJson(content ?? "");
}

// í•œ ë²ˆ í˜¸ì¶œí•´ì„œ ë¬¸ìì—´ë¡œ ë‹µë§Œ ë°›ì•„ì˜¤ëŠ” í—¬í¼
export async function queryKeyword(input_query: string): Promise<any> {
  const response = await xaiClient.chat.completions.create({
    model: "grok-4-fast-non-reasoning",
    messages: [
      { role: "system", content: "You are a helpful assistant." },
      {
        role: "user",
        content: `
Below is the input query of a user. ë‚˜ì¤‘ì— ê²€ìƒ‰ ëª©ë¡ì—ì„œ ë¬´ì—‡ì„ ê²€ìƒ‰í–ˆì—ˆëŠ”ì§€ ë‹¤ì‹œ ê¸°ì–µí•˜ê³  ì°¾ê¸° ì‰½ê²Œ, ì˜ë¯¸ë¥¼ ìœ ì§€í•œì±„ë¡œ 2-3 ë‹¨ì–´ì˜ í‚¤ì›Œë“œë¡œ ë§Œë“¤ì–´ì¤˜.
You should return in korean.

Input Query: ${input_query}

Output:
`,
      },
    ],
  });

  const content = response.choices[0]?.message?.content;
  return content ?? "";
}

const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY! });

type Msg = { role: "system" | "user" | "assistant"; content: string };

function toGeminiContents(messages: Msg[]) {
  // systemì€ config.systemInstructionë¡œ ë„˜ê¸¸ ê±°ë¼ì„œ contentsì—ì„œëŠ” ì œì™¸
  return messages
    .filter((m) => m.role !== "system")
    .map((m) => ({
      role: m.role === "assistant" ? "model" : "user",
      parts: [{ text: m.content ?? "" }],
    }));
}

// model, systemPrompt, messages(=ëŒ€í™” ë©”ì‹œì§€ë“¤)ë§Œ ë°›ì•„ì„œ
// "ReadableStreamìœ¼ë¡œ í…ìŠ¤íŠ¸ deltaë¥¼ í˜ë ¤ë³´ë‚´ëŠ”" í•¨ìˆ˜
export async function geminiChatStream({
  model,
  systemPrompt,
  messages,
  temperature = 0.7,
}: {
  model: string; // e.g. "gemini-2.0-flash"
  systemPrompt: string;
  messages: Msg[];
  temperature?: number;
}) {
  const contents = toGeminiContents([
    { role: "system", content: systemPrompt },
    ...messages,
  ]);

  const stream = await ai.models.generateContentStream({
    model,
    contents,
    config: {
      temperature,
      systemInstruction: systemPrompt,
      thinkingConfig: {
        thinkingLevel: ThinkingLevel.LOW,
      },
    },
  });

  const encoder = new TextEncoder();
  let usage:
  | {
      promptTokenCount?: number;
      candidatesTokenCount?: number;
      totalTokenCount?: number;
    }
  | null = null;
  const responseStream = new ReadableStream<Uint8Array>({
    async start(controller) {
      try {
        for await (const chunk of stream) {
          // ê³µì‹ ì˜ˆì œëŠ” chunk.text ì‚¬ìš©
          const delta =
            (chunk as any).text ??
            chunk.candidates?.[0]?.content?.parts
              ?.map((p: any) => p.text ?? "")
              .join("") ??
            "";

          if (delta) controller.enqueue(encoder.encode(delta));


        // âœ… usageMetadataëŠ” ë³´í†µ ë§ˆì§€ë§‰ chunkì— ìˆìŒ
        if ((chunk as any).usageMetadata) {
          usage = (chunk as any).usageMetadata;
        }
        }
      } catch (error) {
        controller.error(error);
      } finally {
        controller.close();


      // âœ… ì—¬ê¸°ì„œ ë¹„ìš© ë¡œê·¸
      if (usage) {
        console.log("[Gemini usage]", usage);
        // DB ì €ì¥ / credit ì°¨ê° / analytics ì—¬ê¸°ì„œ
      }
      }
    },
  });

  return responseStream;
}
